{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DVfDOcpC7MRq"
   },
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tTcS3A8jUsC-",
    "outputId": "3fa05306-97eb-42bd-dbcb-78161816330f"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bL5Pcw-LUzgu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import Sequential,layers\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization, MaxPool2D, Activation, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPool2D, AveragePooling2D, Lambda, Reshape, UpSampling2D, Conv2DTranspose \n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.models import Model\n",
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "from tensorflow.keras.layers import Conv3D,BatchNormalization, MaxPool3D, Activation, Flatten, Dense, GlobalAveragePooling3D, GlobalMaxPool3D, AveragePooling3D, Lambda\n",
    "\n",
    "import random\n",
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "import datetime\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHNprnNY7MRu"
   },
   "source": [
    "# Dataset Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2TDRLDzQ7MRv"
   },
   "outputs": [],
   "source": [
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)  # X :(145, 145, 30) --> (195, 195, 30) with window =25\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))  # (21025, 25, 25, 30)   \n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))  # (21025,)\n",
    "    patchIndex = 0\n",
    "    \n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]  \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]            \n",
    "            patchIndex = patchIndex + 1\n",
    "  \n",
    "    patchesData = np.expand_dims(patchesData, axis=-1)\n",
    "    return patchesData,patchesLabels\n",
    "\n",
    "def patches_class(X,Y,n):\n",
    "    n_classes = n\n",
    "    patches_list = []\n",
    "    labeles_list = []\n",
    "    for i in range(1,n_classes+1):   # not considering class 0\n",
    "        patchesData_Ith_Label = X[Y==i,:,:,:,:]\n",
    "        Ith_Label = Y[Y==i]\n",
    "        patches_list.append(patchesData_Ith_Label)\n",
    "        labeles_list.append(Ith_Label)\n",
    "        \n",
    "    return patches_list,labeles_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u2Dnqh5A7MRz"
   },
   "outputs": [],
   "source": [
    "windowSize = 11\n",
    "im_height, im_width, im_depth, im_channel = windowSize, windowSize, 30, 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "iMSAOM647MR0",
    "outputId": "0270b57e-2d60-4cbd-a27d-06d1e10db2fc"
   },
   "outputs": [],
   "source": [
    "X = sio.loadmat('/content/drive/MyDrive/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "y = sio.loadmat('/content/drive/MyDrive/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X,pca = applyPCA(X,numComponents=im_depth)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X, y = createImageCubes(X, y, windowSize)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "patches_class_ip,label_ip = patches_class(X,y,16) # class_wise list of patches #(16,) for class 0: (2009, 9, 9, 20, 1)\n",
    "patches_class_ip[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "khF5Rqnt7MR6"
   },
   "source": [
    "#### Dataset split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KbaGmUqg7MR7"
   },
   "outputs": [],
   "source": [
    "train_class_indices = [1,2,4,5,7,9,10,11,13,14]    # 10 classes  \n",
    "train_class_labels = [2,3,5,6,8,10,11,12,14,15]\n",
    "\n",
    "test_class_indices = [0,3,6,8,12,15]               # 6 classes\n",
    "test_class_labels = [1,4,7,9,13,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a954Uss7qb4d"
   },
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MH3SyUEz7MR9"
   },
   "outputs": [],
   "source": [
    "# replace=False - no repeat\n",
    "def new_episode(patches_list,NS,NQ,CS,CQ,class_labels) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = list(np.random.choice(class_labels,CQ,replace=False))  # Randomly choice 6 Query Classes\n",
    "    support_classes = list(np.random.choice(selected_classes,CS,replace=False))  # Randomly choice 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l-zd2ipd7MSI"
   },
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5RVWzPOwBwnM"
   },
   "outputs": [],
   "source": [
    "class Channel_Attention_3D(tf.keras.layers.Layer) :\n",
    "  def __init__(self,C,ratio) :\n",
    "    super(Channel_Attention_3D,self).__init__()\n",
    "    self.avg_pool = GlobalAveragePooling3D()\n",
    "    self.max_pool = GlobalMaxPool3D()\n",
    "    self.activation = Activation('sigmoid')\n",
    "    self.fc1 = Dense(C/ratio, activation = 'relu')\n",
    "    self.fc2 = Dense(C)\n",
    "  def call(self,x) :\n",
    "    avg_out1 = self.avg_pool(x)\n",
    "    avg_out2 = self.fc1(avg_out1)\n",
    "    avg_out3 = self.fc2(avg_out2)\n",
    "    max_out1 = self.max_pool(x)\n",
    "    max_out2 = self.fc1(max_out1)\n",
    "    max_out3 = self.fc2(max_out2)\n",
    "    add_out = tf.math.add(max_out3,avg_out3)\n",
    "    channel_att = self.activation(add_out)\n",
    "    return channel_att "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2F3C5YPE9Bf"
   },
   "outputs": [],
   "source": [
    "class Spatial_Attention_3D(tf.keras.layers.Layer) :\n",
    "  def __init__(self) :\n",
    "    super(Spatial_Attention_3D,self).__init__()\n",
    "    self.conv3d = Conv3D(1,(7,7,7),padding='same',activation='sigmoid')\n",
    "    self.avg_pool_chl = Lambda(lambda x:tf.keras.backend.mean(x,axis=4,keepdims=True))\n",
    "    self.max_pool_chl = Lambda(lambda x:tf.keras.backend.max(x,axis=4,keepdims=True)) \n",
    "  \n",
    "  def call(self,x) :\n",
    "    avg_out1 = self.avg_pool_chl(x)\n",
    "    max_out1 = self.max_pool_chl(x)\n",
    "    concat_out = tf.concat([avg_out1,max_out1],axis=-1)\n",
    "    spatial_att = self.conv3d(concat_out)\n",
    "    return spatial_att "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hONG9s1_FWTn"
   },
   "outputs": [],
   "source": [
    "class CBAM_3D(tf.keras.layers.Layer) :\n",
    "  def __init__(self,C,ratio) :\n",
    "    super(CBAM_3D,self).__init__()\n",
    "    self.C = C\n",
    "    self.ratio = ratio\n",
    "    self.channel_attention = Channel_Attention_3D(self.C,self.ratio)\n",
    "    self.spatial_attention = Spatial_Attention_3D()\n",
    "  def call(self,y,H,W,D,C) :\n",
    "    ch_out1 = self.channel_attention(y)\n",
    "    ch_out2 = tf.expand_dims(ch_out1, axis=1)\n",
    "    ch_out3 = tf.expand_dims(ch_out2, axis=2)\n",
    "    ch_out4 = tf.expand_dims(ch_out3, axis=3)\n",
    "    ch_out5 = tf.tile(ch_out4, multiples=[1,H,W,D,1])\n",
    "    ch_out5 = tf.math.multiply(ch_out5,y)\n",
    "    sp_out1 = self.spatial_attention(ch_out5)\n",
    "    sp_out2 = tf.tile(sp_out1, multiples = [1,1,1,1,C])\n",
    "    sp_out3 = tf.math.multiply(sp_out2,ch_out5)\n",
    "    return sp_out3    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uj7xK8N8EwIq",
    "outputId": "5ac1a399-487b-425e-f967-f57df274f3de"
   },
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape = (im_height, im_width, im_depth, im_channel))\n",
    "out1 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(input_layer)\n",
    "out2 = CBAM_3D(out1.shape[4],4)(out1,out1.shape[1],out1.shape[2],out1.shape[3],out1.shape[4])\n",
    "out2 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out1)\n",
    "out2 = CBAM_3D(out2.shape[4],4)(out2,out2.shape[1],out2.shape[2],out2.shape[3],out2.shape[4])\n",
    "out3 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out2)\n",
    "out4 = layers.Add()([out1, out3])  #Concatenate()\n",
    "out5 = layers.MaxPool3D(pool_size=(2, 2, 4), strides=None, padding='same')(out4)\n",
    "\n",
    "out6 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out5)\n",
    "out6 = CBAM_3D(out6.shape[4],4)(out6,out6.shape[1],out6.shape[2],out6.shape[3],out6.shape[4])\n",
    "out7 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out6)\n",
    "out7 = CBAM_3D(out7.shape[4],4)(out7,out7.shape[1],out7.shape[2],out7.shape[3],out7.shape[4])\n",
    "out8 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out7)\n",
    "out9 = layers.Add()([out6, out8])  #Concatenate()\n",
    "out10 = layers.MaxPool3D(pool_size=(2, 2, 2), strides=None, padding='same')(out9)\n",
    "out10 = CBAM_3D(out10.shape[4],4)(out10,out10.shape[1],out10.shape[2],out10.shape[3],out10.shape[4])\n",
    "out11 = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='valid',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out10)\n",
    "out12 = layers.Flatten()(out11)\n",
    "FE_model = Model(inputs=input_layer,outputs=out12,name='3DResCNN')\n",
    "FE_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9AV1ZGobW-H8"
   },
   "source": [
    "**VAE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fc2JyiTdroNl"
   },
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "batch_size = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vUlkGYWir3om"
   },
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "  z_mean, z_log_sigma = args\n",
    "  epsilon = tf.random.normal([latent_dim,], 0, 1, tf.float32)  #batch_size\n",
    "  return z_mean + tf.math.exp(z_log_sigma) * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "llhS56MAXA7Q",
    "outputId": "61e01c75-1409-4c36-bfce-ae83037acb19"
   },
   "outputs": [],
   "source": [
    "input_Feature = layers.Input(shape = (64,))\n",
    "encoded_L1 = layers.Dense(32, activation='relu')(input_Feature)\n",
    "encoded_L2 = layers.Dense(16, activation='relu')(encoded_L1)\n",
    "\n",
    "z_mean = layers.Dense(latent_dim)(encoded_L2)\n",
    "z_log_sigma = layers.Dense(latent_dim)(encoded_L2)\n",
    "z = Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_sigma])\n",
    "\n",
    "decoded_h1 = layers.Dense(16, activation='relu')\n",
    "decoded_h2 = layers.Dense(32, activation='relu')\n",
    "decoded_mean = layers.Dense(64, activation='relu')\n",
    "\n",
    "decoded_L1 =  decoded_h1(z)\n",
    "decoded_L2 =  decoded_h2(decoded_L1)\n",
    "decoded_X =  decoded_mean(decoded_L2)\n",
    "VAE = Model(inputs=input_Feature,outputs=[decoded_X,z_mean,z_log_sigma],name='VAE')\n",
    "VAE.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DxvVeVVzzn_A"
   },
   "outputs": [],
   "source": [
    "def vae_loss(x, x_decoded_mean, z_mean, z_log_sigma):\n",
    "  #Recon_loss = tf.reduce_mean(tf.keras.losses.binary_crossentropy(x, x_decoded_mean))\n",
    "  recon_loss = tf.reduce_mean(tf.math.square(tf.math.subtract(x,x_decoded_mean)))\n",
    "  kl_loss = - 0.5 * tf.reduce_mean(1 + z_log_sigma - tf.math.square(z_mean) - tf.math.exp(z_log_sigma))\n",
    "  return recon_loss + kl_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQhTXdgyMHgP"
   },
   "outputs": [],
   "source": [
    "CQ = 6 #C2\n",
    "CS = 3 #C1\n",
    "N = 15\n",
    "K = 5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.00001)   #1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-iRHvh1JsVY"
   },
   "source": [
    "Outlier NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jsXP2tgjJrgm",
    "outputId": "617d35bf-05c8-440f-a4a8-44465bd22deb"
   },
   "outputs": [],
   "source": [
    "input_Feature = layers.Input(shape = (3,))\n",
    "encoded_L1 = layers.Dense(16, activation='relu')(input_Feature)\n",
    "encoded_L2 = layers.Dense(8, activation='relu')(encoded_L1)\n",
    "decoded = layers.Dense(2, activation='softmax')(encoded_L2)\n",
    "outlier_nn = Model(inputs=input_Feature,outputs=decoded,name='ONN')\n",
    "outlier_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb_vFZbnLmCP"
   },
   "source": [
    "# Protototypical Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_nfkYMHw-Xco"
   },
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "  # x : (n,d)\n",
    "  # y : (m,d)\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cJLcFCJ8vwTy"
   },
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optim1 = tf.keras.optimizers.Adam(0.0001) \n",
    "optim2 = tf.keras.optimizers.Adam(0.0001) \n",
    "optim3 = tf.keras.optimizers.Adam(0.0001) \n",
    "checkpoint_dir = '/content/drive/MyDrive/Hyperspectral OSR/CBAM_ResCNN_VAE_CLF_new'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optim1=optim1, optim2=optim2, optim3=optim3, FE_model = FE_model, VAE = VAE, outlier_nn=outlier_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ql1OKEpzUf_B"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "dKIG82OqtL6p",
    "outputId": "fdfb8fbd-a5a3-4edb-ee5f-6057ac4f4b88"
   },
   "outputs": [],
   "source": [
    "ntimes = 10\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def proto_train(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,K,CS,CQ,N):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 64]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 64]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "      if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    z_prototypes = tf.reshape(sembed,[CS, K, sembed.shape[-1]])           # [3, 5, 64]\n",
    "    z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)        # [3, 64]   \n",
    "    # Vautoencoder Loss on Query + Support\n",
    "    rec_kl_loss = 0\n",
    "    clf_loss = 0\n",
    "    sqembedK = np.zeros((CS*(N+K),emb_dim)) #known query then support samples\n",
    "    y_sqK = np.asarray(np.zeros((CS*(N+K),CS)),dtype=np.float32) # QK + SK\n",
    "    j = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "      if np.sum(y_query[i,:])==1:     # k query \n",
    "        y_sqK[j,:] = y_query[i,:]\n",
    "        sqembedK[j,:] = qembed[i,:]\n",
    "        j = j + 1\n",
    "    for i in range(len(sembed)) :\n",
    "      sqembedK[j,:] = sembed[i,:]\n",
    "      y_sqK[j,:] = y_support[i,:]\n",
    "      j = j + 1\n",
    "\n",
    "    with tf.GradientTape() as VAEtape:\n",
    "      for n in range(ntimes):\n",
    "          gen_sqembedK, z_mean, z_log_sigma = VAE(sqembedK)\n",
    "          rec_kl_loss = rec_kl_loss + vae_loss(sqembedK,gen_sqembedK, z_mean, z_log_sigma)\n",
    "          dists_genK = calc_euclidian_dists(gen_sqembedK, z_prototypes) \n",
    "          log_p_y_genK = tf.nn.log_softmax(-dists_genK,axis=-1)\n",
    "          clf_loss = clf_loss - tf.reduce_mean((tf.reduce_sum(tf.multiply(y_sqK, log_p_y_genK), axis=-1))) \n",
    "      total_loss = clf_loss+rec_kl_loss\n",
    "    grads = VAEtape.gradient(total_loss, VAE.trainable_variables)\n",
    "    optim1.apply_gradients(zip(grads, VAE.trainable_variables))\n",
    "    \n",
    "    # Query set Augmentation((S + QK)(Original + Gen) + QU) [CEC loss for FE]\n",
    "    sqembed_gen_K, mean, sigma = VAE(sqembedK)\n",
    "    y_sq_Aug = y_sqK            # (Q+S)K + (Q+S)K(Gen) + QU\n",
    "    sqembed_Aug = sqembedK              # query + support knowns\n",
    "    sqembed_Aug = tf.concat((sqembed_Aug,sqembed_gen_K),axis=0)  # stacking known [k class]>AE o/p  \n",
    "    y_sq_Aug = tf.concat((y_sq_Aug,y_sqK),axis=0)\n",
    "    for i in range(len(y_query)):     # 90\n",
    "      if np.sum(y_query[i,:])==0:     # u query \n",
    "        y_sq_Aug = tf.concat((y_sq_Aug,tf.expand_dims(y_query[i,:],axis=0)),axis=0)\n",
    "        sqembed_Aug = tf.concat((sqembed_Aug,tf.expand_dims(qembed[i,:],axis=0)),axis=0)\n",
    "    dists_Aug = calc_euclidian_dists(sqembed_Aug, z_prototypes) \n",
    "    log_p_y_Aug = tf.nn.log_softmax(-dists_Aug,axis=-1)\n",
    "    cec_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_sq_Aug, log_p_y_Aug), axis=-1))) \n",
    "    \n",
    "    #outlier detection [outlier network update]\n",
    "    with tf.GradientTape() as outlier_tape:\n",
    "      outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "      y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "      for i in range(len(y_sq_Aug)) :\n",
    "        if i < 2*(CS*(K+N)):\n",
    "          y_outlier[i] = 1\n",
    "      outlier_loss = 10*scce(y_outlier,outlier_pred)\n",
    "    grads = outlier_tape.gradient(outlier_loss, outlier_nn.trainable_variables)\n",
    "    optim2.apply_gradients(zip(grads, outlier_nn.trainable_variables))\n",
    "    \n",
    "    # outlier loss calculation for FE update\n",
    "    outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "    y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "    for i in range(len(y_sq_Aug)) :\n",
    "      if i < 2*(CS*(K+N)):\n",
    "        y_outlier[i] = 1\n",
    "    outlier_loss = 10*scce(y_outlier,outlier_pred)\n",
    "\n",
    "    #accuracy calculation\n",
    "    correct_pred = 0\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)               # [90, 3]  \n",
    "    outlier_prob = outlier_nn(dists)\n",
    "    outlier_index = tf.argmax(outlier_prob,axis=-1)\n",
    "    predictions = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred = predictions\n",
    "    pred2 = predictions\n",
    "    pred_index = tf.argmax(predictions,axis=-1)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "        if outlier_index[i] == 1 :\n",
    "          x = support_classes.index(ep_query_labels[i])\n",
    "          if x == pred_index[i] :\n",
    "            correct_pred += 1  \n",
    "      else :\n",
    "          if outlier_index[i] == 0 :\n",
    "            outlier = outlier + 1  \n",
    "    accuracy = correct_pred/(CS*N)     # scalar      \n",
    "    outlier_det_acc = outlier/((CQ-CS)*N)\n",
    "\n",
    "    #open oa\n",
    "    y_pred = np.zeros((len(ep_query_labels))) \n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if outlier_index[i] == 1 :\n",
    "          y_pred[i] = pred_index[i]+1\n",
    "        else :\n",
    "          y_pred[i] = 0\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "      \n",
    "    loss = cec_loss + outlier_loss\n",
    "    return loss, accuracy, outlier_det_acc, open_oa    # scalar, scalar\n",
    "\n",
    "# Metrics to gather\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "train_openoa = tf.metrics.Mean(name='train_openoa')\n",
    "train_outlier_acc = tf.metrics.Mean(name='train_outlier_acc')\n",
    "\n",
    "def train_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, accuracy, outlier_det_acc, openoa = proto_train(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N)\n",
    "    gradients = tape.gradient(loss, FE_model.trainable_variables)\n",
    "    optim3.apply_gradients(zip(gradients, FE_model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_acc(accuracy)\n",
    "    train_openoa(openoa)\n",
    "    train_outlier_acc(outlier_det_acc)\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "for epoch in range(5001): # 80 train + 80 tune + 100 train + 160 tune + 40 train\n",
    "    train_loss.reset_states()  \n",
    "    train_acc.reset_states()\n",
    "    train_openoa.reset_states()\n",
    "    train_outlier_acc.reset_states()\n",
    "    for epi in range(10): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = new_episode(patches_class_ip,K,N,CS,CQ,train_class_labels)   \n",
    "        train_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N)\n",
    "        \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('openoa',train_openoa.result(), step=epoch)\n",
    "        tf.summary.scalar('outlier_det_acc',train_outlier_acc.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Train Loss: {:.2f}, Train Accuracy: {:.2f}, Train Open OA: {:.2f}, Train Outlier Det. Acc: {:.2f}'\n",
    "    print(template.format(epoch+1,train_loss.result(),train_acc.result()*100,train_openoa.result()*100,train_outlier_acc.result()*100))\n",
    "\n",
    "    if epoch % 500 == 0 and epoch != 0 :\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bvzcFUWUNZIm"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs/gradient_tape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNyrRuhMON8o"
   },
   "source": [
    "# Fine-tuning\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aS224QxqTYNn"
   },
   "outputs": [],
   "source": [
    "checkpoint_dir_tune = '/content/drive/MyDrive/Hyperspectral OSR/New_IP_ckpts'\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir_tune, \"ckpt\")\n",
    "checkpoint_tune = tf.train.Checkpoint(optimizer=optimizer,FE_model = FE_model, VAE = VAE, outlier_nn=outlier_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nzvKxHabyVJQ"
   },
   "outputs": [],
   "source": [
    "train_class_indices = [1,2,4,5,7,9,10,11,13,14]\n",
    "test_class_indices = [0,3,6,8,12,15]\n",
    "train_patches_class = [patches_class_ip[i] for i in train_class_indices]        #(10)\n",
    "test_patches_class = [patches_class_ip[i] for i in test_class_indices]        #(6) \n",
    "train_class_labels = [2,3,5,6,8,10,11,12,14,15]   \n",
    "test_class_labels = [1,4,7,9,13,16]     #[11...16]\n",
    "test_support_labels = [16,4,13]\n",
    "ft_labels = [2,3,4,5,6,8,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f64ExXqB_J_F"
   },
   "outputs": [],
   "source": [
    "tune_set_5 = [[] for i in range(16)]\n",
    "for j in range(1,17) :\n",
    "  if j in train_class_labels :\n",
    "    tune_set_5[j-1] = patches_class_ip[j-1] \n",
    "  elif j in test_support_labels :\n",
    "    tune_set_5[j-1] = patches_class_ip[j-1][:5,:,:,:,:] # for each class first 5 samples taken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvON3Mk8HZQg"
   },
   "outputs": [],
   "source": [
    "def tune_episode(patches_list,NS,NQ,CS,CQ,class_labels) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = list(np.random.choice(class_labels,CQ,replace=False))  # Randomly choice 6 Query Classes\n",
    "    support_classes = list(np.random.choice(selected_classes,CS,replace=False))  # Randomly choice 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "78mUqPSj3DA6"
   },
   "outputs": [],
   "source": [
    "tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = tune_episode(tune_set_5,1,4,3,6,ft_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6xqmgxfiOaoU",
    "outputId": "078836e8-6014-4e05-cef9-a9e455627199"
   },
   "outputs": [],
   "source": [
    "emb_dim = 64\n",
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "ntimes = 10\n",
    "tK = 1\n",
    "tN = 4\n",
    "optim1 = tf.keras.optimizers.Adam(0.00001) \n",
    "optim2 = tf.keras.optimizers.Adam(0.00001) \n",
    "optim3 = tf.keras.optimizers.Adam(0.00001)\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "def proto_tune(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,tK,CS,CQ,tN):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 64]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 64]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "      if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    z_prototypes = tf.reshape(sembed,[CS, tK, sembed.shape[-1]])           # [3, 5, 64]\n",
    "    z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)        # [3, 64]   \n",
    "    # Vautoencoder Loss on Query + Support\n",
    "    rec_kl_loss = 0\n",
    "    clf_loss = 0\n",
    "    sqembedK = np.zeros((CS*(tN+tK),emb_dim)) #known query then support samples\n",
    "    y_sqK = np.asarray(np.zeros((CS*(tN+tK),CS)),dtype=np.float32) # QK + SK\n",
    "    j = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "      if np.sum(y_query[i,:])==1:     # k query \n",
    "        y_sqK[j,:] = y_query[i,:]\n",
    "        sqembedK[j,:] = qembed[i,:]\n",
    "        j = j + 1\n",
    "    for i in range(len(sembed)) :\n",
    "      sqembedK[j,:] = sembed[i,:]\n",
    "      y_sqK[j,:] = y_support[i,:]\n",
    "      j = j + 1\n",
    "\n",
    "    with tf.GradientTape() as VAEtape:\n",
    "      for n in range(ntimes):\n",
    "          gen_sqembedK, z_mean, z_log_sigma = VAE(sqembedK)\n",
    "          rec_kl_loss = rec_kl_loss + vae_loss(sqembedK,gen_sqembedK, z_mean, z_log_sigma)\n",
    "          dists_genK = calc_euclidian_dists(gen_sqembedK, z_prototypes) \n",
    "          log_p_y_genK = tf.nn.log_softmax(-dists_genK,axis=-1)\n",
    "          clf_loss = clf_loss - tf.reduce_mean((tf.reduce_sum(tf.multiply(y_sqK, log_p_y_genK), axis=-1))) \n",
    "      total_loss = clf_loss+rec_kl_loss\n",
    "    grads = VAEtape.gradient(total_loss, VAE.trainable_variables)\n",
    "    optim1.apply_gradients(zip(grads, VAE.trainable_variables))\n",
    "    \n",
    "    # Query set Augmentation((S + QK)(Original + Gen)) [CEC loss for FE]\n",
    "    sqembed_gen_K, mean, sigma = VAE(sqembedK)\n",
    "    y_sq_Aug = y_sqK            # (Q+S)K + (Q+S)K(Gen) + QU\n",
    "    sqembed_Aug = sqembedK              # query + support knowns\n",
    "    sqembed_Aug = tf.concat((sqembed_Aug,sqembed_gen_K),axis=0)  # stacking known [k class]>AE o/p  \n",
    "    y_sq_Aug = tf.concat((y_sq_Aug,y_sqK),axis=0)\n",
    "    for i in range(len(y_query)):     # 90\n",
    "      if np.sum(y_query[i,:])==0:     # u query \n",
    "        y_sq_Aug = tf.concat((y_sq_Aug,tf.expand_dims(y_query[i,:],axis=0)),axis=0)\n",
    "        sqembed_Aug = tf.concat((sqembed_Aug,tf.expand_dims(qembed[i,:],axis=0)),axis=0)\n",
    "    dists_Aug = calc_euclidian_dists(sqembed_Aug, z_prototypes) \n",
    "    log_p_y_Aug = tf.nn.log_softmax(-dists_Aug,axis=-1)\n",
    "    cec_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_sq_Aug, log_p_y_Aug), axis=-1))) \n",
    "    #print(len(y_sq_Aug))\n",
    "    \n",
    "    #outlier detection [outlier network update]\n",
    "    with tf.GradientTape() as outlier_tape:\n",
    "      outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "      y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "      for i in range(len(y_sq_Aug)) :\n",
    "        if i < 2*(CS*(tK+tN)):\n",
    "          y_outlier[i] = 1\n",
    "      outlier_loss = 10*scce(y_outlier,outlier_pred)\n",
    "    grads = outlier_tape.gradient(outlier_loss, outlier_nn.trainable_variables)\n",
    "    optim2.apply_gradients(zip(grads, outlier_nn.trainable_variables))\n",
    "    \n",
    "    # outlier loss calculation for FE update\n",
    "    outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "    y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "    for i in range(len(y_sq_Aug)) :\n",
    "      if i < 2*(CS*(tK+tN)):\n",
    "        y_outlier[i] = 1\n",
    "    outlier_loss = 10*scce(y_outlier,outlier_pred)\n",
    "\n",
    "    #accuracy calculation\n",
    "    correct_pred = 0\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)               # [90, 3]  \n",
    "    outlier_prob = outlier_nn(dists)\n",
    "    outlier_index = tf.argmax(outlier_prob,axis=-1)\n",
    "    predictions = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred = predictions\n",
    "    pred2 = predictions\n",
    "    pred_index = tf.argmax(predictions,axis=-1)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "        if outlier_index[i] == 1 :\n",
    "          x = support_classes.index(ep_query_labels[i])\n",
    "          if x == pred_index[i] :\n",
    "            correct_pred += 1  \n",
    "      else :\n",
    "          if outlier_index[i] == 0 :\n",
    "            outlier = outlier + 1  \n",
    "    accuracy = correct_pred/(CS*tN)     # scalar      \n",
    "    outlier_det_acc = outlier/((CQ-CS)*tN)\n",
    "\n",
    "\n",
    "    #open oa\n",
    "    y_pred = np.zeros((len(ep_query_labels))) \n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if outlier_index[i] == 1 :\n",
    "          y_pred[i] = pred_index[i]+1\n",
    "        else :\n",
    "          y_pred[i] = 0\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "      \n",
    "    loss = cec_loss + outlier_loss\n",
    "    return loss, accuracy, outlier_det_acc, open_oa    # scalar, scalar\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(0.00001) \n",
    "# Metrics to gather\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "tune_open_acc = tf.metrics.Mean(name='tune_open_accuracy')\n",
    "tune_outlier_acc = tf.metrics.Mean(name='tune_outlier_accuracy')\n",
    "\n",
    "def tune_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, accuracy, outlier_det_acc, open_oa = proto_tune(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN)\n",
    "    gradients = tape.gradient(loss, FE_model.trainable_variables)\n",
    "    optim3.apply_gradients(zip(gradients, FE_model.trainable_variables))\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(accuracy)\n",
    "    tune_open_acc(open_oa)\n",
    "    tune_outlier_acc(outlier_det_acc)\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tune_log_dir = 'logs/gradient_tape/' + current_time + '/tune'\n",
    "tune_summary_writer = tf.summary.create_file_writer(tune_log_dir)\n",
    "        \n",
    "for epoch in range(1001): \n",
    "    tune_loss.reset_states()  \n",
    "    tune_acc.reset_states()\n",
    "    tune_open_acc.reset_states()\n",
    "    tune_outlier_acc.reset_states()\n",
    "    for epi in range(10): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = tune_episode(tune_set_5,1,4,3,6,ft_labels)   \n",
    "        tune_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN)\n",
    "    \n",
    "    with tune_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', tune_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', tune_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('Open_accuracy', tune_open_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('Outlier_accuracy', tune_outlier_acc.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}, Open Accuracy: {:.2f},Outlier Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,tune_loss.result(),tune_acc.result()*100,tune_open_acc.result()*100,tune_outlier_acc.result()*100))\n",
    "\n",
    "    if epoch % 100 == 0 and epoch != 0 :\n",
    "      checkpoint_tune.save(file_prefix = checkpoint_prefix_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PhBOXStmwFzo"
   },
   "source": [
    "# Testing after finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YhckI8xj7GIg"
   },
   "outputs": [],
   "source": [
    "def proto_test(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,K,CS,CQ,N):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 64]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 64]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes\n",
    "    y_auc = np.zeros((len(ep_query_labels))) #for storing labels, 1 for seen, and 0 for unseen\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "            y_auc[i] = 1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "      if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    z_prototypes = tf.reshape(sembed,[CS, K, sembed.shape[-1]])           # [3, 5, 64]\n",
    "    z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)        # [3, 64]   \n",
    "\n",
    "    sqembedK = np.zeros((CS*(N+K),emb_dim)) #known query then support samples\n",
    "    y_sqK = np.asarray(np.zeros((CS*(N+K),CS)),dtype=np.float32) # QK + SK\n",
    "    j = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "      if np.sum(y_query[i,:])==1:     # k query \n",
    "        y_sqK[j,:] = y_query[i,:]\n",
    "        sqembedK[j,:] = qembed[i,:]\n",
    "        j = j + 1\n",
    "    for i in range(len(sembed)) :\n",
    "      sqembedK[j,:] = sembed[i,:]\n",
    "      y_sqK[j,:] = y_support[i,:]\n",
    "      j = j + 1\n",
    "    \n",
    "    #RE calculation\n",
    "    qembedK = np.zeros((CS*N,emb_dim))\n",
    "    qembedU = np.zeros(((CQ-CS)*N,emb_dim))\n",
    "    y_queryK = np.zeros((CS*N,CS))\n",
    "    j = 0\n",
    "    k = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "      if np.sum(y_query[i,:])==1:     # k query \n",
    "        y_queryK[j,:] = y_query[i,:]\n",
    "        qembedK[j,:] = qembed[i,:]\n",
    "        j = j + 1\n",
    "      else :\n",
    "        qembedU[k,:] = qembed[i,:]\n",
    "        k = k + 1\n",
    "    gen_qembedK, z_mean, z_log_sigma = VAE(qembedK)\n",
    "    gen_qembedU, z_mean, z_log_sigma = VAE(qembedU)\n",
    "    rec_loss_k = abs(tf.reduce_mean(gen_qembedK-qembedK))\n",
    "    rec_loss_u = abs(tf.reduce_mean(gen_qembedU-qembedU))  \n",
    "\n",
    "    #accuracy calculation\n",
    "    correct_pred = 0\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)               # [90, 3]  \n",
    "    outlier_prob = outlier_nn(dists)\n",
    "    outlier_prob1 = outlier_prob\n",
    "    outlier_index = tf.argmax(outlier_prob,axis=-1)\n",
    "    predictions = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred = predictions\n",
    "    pred2 = predictions\n",
    "    pred_index = tf.argmax(predictions,axis=-1)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      if ep_query_labels[i] in support_classes :\n",
    "        if outlier_index[i] == 1 :\n",
    "          x = support_classes.index(ep_query_labels[i])\n",
    "          if x == pred_index[i] :\n",
    "            correct_pred += 1  \n",
    "      else :\n",
    "          if outlier_index[i] == 0 :\n",
    "            outlier = outlier + 1  \n",
    "    accuracy = correct_pred/(CS*N)     # scalar      \n",
    "    outlier_det_acc = outlier/((CQ-CS)*N)\n",
    "    y_score = np.zeros((len(ep_query_labels)))\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "      y_score[i] = outlier_prob1[i,1]\n",
    "    auc = sklearn.metrics.roc_auc_score(y_auc, y_score)\n",
    "\n",
    "\n",
    "    #open oa\n",
    "    y_pred = np.zeros((len(ep_query_labels))) \n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if outlier_index[i] == 1 :\n",
    "          y_pred[i] = pred_index[i]+1\n",
    "        else :\n",
    "          y_pred[i] = 0\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))    \n",
    "    return accuracy, open_oa, outlier_det_acc, auc    # scalar, scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VBuKCukctlvN"
   },
   "outputs": [],
   "source": [
    "def test_episode(patches_list,NS,NQ,CS,CQ) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = test_class_labels  # 6 Query Classes\n",
    "    support_classes = test_support_labels  # 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gAK3r6MTwBld",
    "outputId": "ac623140-83de-4e6f-89cc-4dd7cf9c38a4"
   },
   "outputs": [],
   "source": [
    "total_acc = 0 \n",
    "total_open_oa = 0\n",
    "total_outlier_acc = 0 \n",
    "rec_k = 0\n",
    "rec_u = 0\n",
    "f1score = 0 \n",
    "total_auc = 0\n",
    "emb_dim = 64\n",
    "tepochs = 100\n",
    "K = 5\n",
    "N = 15\n",
    "for i in range(tepochs) :\n",
    "  tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = test_episode(patches_class_ip,5,15,3,6)   \n",
    "  accuracy, open_oa, outlier_det_acc, auc = proto_test(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,5,3,6,15)\n",
    "  total_acc = total_acc + accuracy\n",
    "  total_open_oa = total_open_oa + open_oa\n",
    "  total_outlier_acc = total_outlier_acc + outlier_det_acc\n",
    "  total_auc = total_auc + auc\n",
    "print('accuracy',total_acc*100/tepochs)\n",
    "print('Outlier detection accuracy', (total_outlier_acc*100/tepochs))\n",
    "print('open oa',total_open_oa*100/tepochs)\n",
    "print('auc',total_auc/tepochs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "IP_CBAM_Hyperspectral_OSR_VAE_Prototypical_cec_FE_clf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
